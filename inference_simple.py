# simple_storm_inference.py
import os
import torch
import torchaudio
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
import sys
from typing import Optional, Union

from storm_model import StoRMModel


class SimpleStoRMInference:
    """
    ç®€åŒ–çš„StoRMæ¨ç†å™¨
    """
    
    def __init__(self, 
                 model_path: str,
                 device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                 sr: int = 16000):
        
        self.device = device
        self.sr = sr
        
        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        self.model = self._load_model(model_path)
        self.model.eval()
        
        # STFTå‚æ•°
        self.n_fft = 510
        self.hop_length = 128
        self.win_length = 510
        
        print(f"æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ (è®¾å¤‡: {device}, é‡‡æ ·ç‡: {sr}Hz)")
    
    def _load_model(self, model_path: str) -> StoRMModel:
        """åŠ è½½æ¨¡å‹"""
        model = StoRMModel(base_channels=32).to(self.device)
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if 'ema_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['ema_state_dict'])
            print("  ä½¿ç”¨EMAæ¨¡å‹æƒé‡")
        else:
            model.load_state_dict(checkpoint['model_state_dict'])
            print("  ä½¿ç”¨æ™®é€šæ¨¡å‹æƒé‡")
        
        return model
    
    def _preprocess_audio(self, waveform: torch.Tensor, input_sr: int) -> torch.Tensor:
        """é¢„å¤„ç†éŸ³é¢‘ - ç®€åŒ–ç‰ˆæœ¬"""
        # ç®€åŒ–ï¼šç¡®ä¿æ˜¯2D [channels, samples]
        if waveform.dim() == 1:
            waveform = waveform.unsqueeze(0)  # [samples] -> [1, samples]
        elif waveform.dim() == 3:
            # [batch, channels, samples] -> [channels, samples]
            waveform = waveform.squeeze(0)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # é‡é‡‡æ ·
        if input_sr != self.sr:
            waveform = torchaudio.functional.resample(
                waveform, 
                orig_freq=input_sr, 
                new_freq=self.sr
            )
        
        return waveform
    
    def _simple_stft(self, waveform: torch.Tensor):
        """ç®€åŒ–çš„STFT"""
        # ç¡®ä¿æ˜¯2D [batch, samples]
        if waveform.dim() == 1:
            waveform = waveform.unsqueeze(0)
        
        window = torch.hann_window(self.win_length)
        
        stft = torch.stft(
            waveform,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_length,
            window=window,
            return_complex=True
        )
        
        # è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼ [batch, 2, freq, time]
        real = stft.real.unsqueeze(1)
        imag = stft.imag.unsqueeze(1)
        
        return torch.cat([real, imag], dim=1)
    
    def _simple_istft(self, complex_spec: torch.Tensor, target_length: Optional[int] = None):
        """ç®€åŒ–çš„ISTFT"""
        from typing import Optional
        
        # æå–å®éƒ¨å’Œè™šéƒ¨
        real = complex_spec[:, 0, :, :]
        imag = complex_spec[:, 1, :, :]
        
        # åˆ›å»ºå¤æ•°
        stft_complex = torch.complex(real, imag)
        
        window = torch.hann_window(self.win_length)
        
        # è®¡ç®—è¾“å‡ºé•¿åº¦
        freq_bins = complex_spec.shape[2]
        calculated_length = (freq_bins - 1) * self.hop_length
        
        # ä½¿ç”¨ç›®æ ‡é•¿åº¦æˆ–è®¡ç®—é•¿åº¦
        output_length = target_length if target_length is not None else calculated_length
        
        waveform = torch.istft(
            stft_complex,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_length,
            window=window,
            length=output_length
        )
        
        return waveform
    
    def enhance_audio(self, 
                     noisy_waveform: torch.Tensor,
                     input_sr: int = 16000,
                     denoise_only: bool = True,
                     num_steps: int = 10) -> torch.Tensor:
        """
        å¢å¼ºéŸ³é¢‘ - æœ€ç®€å•å¯é çš„ç‰ˆæœ¬
        
        Args:
            noisy_waveform: è¾“å…¥æ³¢å½¢
            input_sr: è¾“å…¥é‡‡æ ·ç‡
            denoise_only: æ˜¯å¦ä»…ä½¿ç”¨åˆ¤åˆ«æ¨¡å‹
            num_steps: æ‰©æ•£æ­¥æ•°ï¼ˆä»…å½“denoise_only=Falseæ—¶æœ‰æ•ˆï¼‰
        """
        print(f"\nå¼€å§‹å¢å¼ºéŸ³é¢‘...")
        
        with torch.no_grad():
            # 1. é¢„å¤„ç†
            waveform = self._preprocess_audio(noisy_waveform, input_sr)
            original_length = waveform.shape[1]
            print(f"åŸå§‹é•¿åº¦: {original_length}æ ·æœ¬ ({original_length/self.sr:.3f}ç§’)")
            
            # 2. STFT
            stft = self._simple_stft(waveform)
            print(f"STFTå½¢çŠ¶: {stft.shape}")
            
            # 3. è°ƒæ•´å°ºå¯¸ä¸º8çš„å€æ•°
            B, C, F, T = stft.shape
            if F % 8 != 0 or T % 8 != 0:
                target_F = ((F + 7) // 8) * 8
                target_T = ((T + 7) // 8) * 8
                stft = torch.nn.functional.interpolate(
                    stft,
                    size=(target_F, target_T),
                    mode='bilinear',
                    align_corners=False
                )
                print(f"è°ƒæ•´STFT: {stft.shape}")
            
            # 4. æ¨¡å‹å¢å¼º
            stft = stft.to(self.device)
            
            if denoise_only:
                enhanced_stft = self.model.enhance(stft, denoise_only=True)
                print(f"ä½¿ç”¨åˆ¤åˆ«æ¨¡å‹å¢å¼º")
            else:
                enhanced_stft = self.model.enhance(stft, num_steps=num_steps, denoise_only=False)
                print(f"ä½¿ç”¨æ‰©æ•£æ¨¡å‹å¢å¼º (æ­¥æ•°: {num_steps})")
            
            print(f"å¢å¼ºSTFT: {enhanced_stft.shape}")
            
            # 5. æ¢å¤åŸå§‹STFTå°ºå¯¸
            enhanced_stft = enhanced_stft.cpu()
            if enhanced_stft.shape[2:] != (F, T):
                enhanced_stft = torch.nn.functional.interpolate(
                    enhanced_stft,
                    size=(F, T),
                    mode='bilinear',
                    align_corners=False
                )
            
            # 6. ISTFT
            enhanced_waveform = self._simple_istft(enhanced_stft, original_length)
            print(f"å¢å¼ºæ³¢å½¢: {enhanced_waveform.shape}")
            
            # 7. ç¡®ä¿æ­£ç¡®é•¿åº¦
            current_length = enhanced_waveform.shape[1]
            if current_length != original_length:
                if current_length > original_length:
                    enhanced_waveform = enhanced_waveform[:, :original_length]
                else:
                    padding = torch.zeros(1, original_length - current_length)
                    enhanced_waveform = torch.cat([enhanced_waveform, padding], dim=1)
            
            print(f"æœ€ç»ˆæ³¢å½¢: {enhanced_waveform.shape}")
            print(f"å¢å¼ºå®Œæˆ: {enhanced_waveform.shape[1]}æ ·æœ¬ ({enhanced_waveform.shape[1]/self.sr:.3f}ç§’)")
            
            return enhanced_waveform.squeeze()
    
    def process_file(self, 
                    input_path: str, 
                    output_path: str,
                    denoise_only: bool = True,
                    num_steps: int = 10,
                    verbose: bool = True) -> bool:
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"å¤„ç†æ–‡ä»¶: {input_path}")
            print(f"{'='*60}")
        
        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sr = torchaudio.load(input_path)
            if verbose:
                print(f"åŠ è½½éŸ³é¢‘: {waveform.shape}, {sr}Hz")
            
            # å¢å¼ºéŸ³é¢‘
            enhanced = self.enhance_audio(
                waveform, 
                input_sr=sr, 
                denoise_only=denoise_only,
                num_steps=num_steps
            )
            
            # ç¡®ä¿æ­£ç¡®çš„ç»´åº¦
            if enhanced.dim() == 1:
                enhanced = enhanced.unsqueeze(0)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = os.path.dirname(output_path)
            if output_dir:
                os.makedirs(output_dir, exist_ok=True)
            
            # ä¿å­˜ç»“æœ
            torchaudio.save(output_path, enhanced, self.sr)
            
            if verbose:
                # éªŒè¯
                if os.path.exists(output_path):
                    loaded, loaded_sr = torchaudio.load(output_path)
                    duration = loaded.shape[1] / loaded_sr
                    print(f"\nâœ… å¤„ç†æˆåŠŸ!")
                    print(f"  ä¿å­˜åˆ°: {output_path}")
                    print(f"  æ—¶é•¿: {duration:.2f}ç§’")
                    print(f"  å¤§å°: {os.path.getsize(output_path) / 1024:.1f}KB")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            return False
    
    def process_directory(self,
                        input_dir: Union[str, Path],
                        output_dir: Union[str, Path],
                        file_ext: str = '.wav',
                        denoise_only: bool = True,
                        num_steps: int = 10,
                        suffix: str = '_enhanced') -> dict:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„éŸ³é¢‘æ–‡ä»¶
        """
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = list(input_dir.glob(f'*{file_ext}'))
        if not audio_files:
            audio_files = list(input_dir.rglob(f'*{file_ext}'))
        
        print(f"\næ‰¹é‡å¤„ç†:")
        print(f"  è¾“å…¥ç›®å½•: {input_dir}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")
        print(f"  æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        
        # å¤„ç†ç»Ÿè®¡
        stats = {
            'total': len(audio_files),
            'success': 0,
            'failed': 0
        }
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for audio_file in tqdm(audio_files, desc="å¤„ç†éŸ³é¢‘æ–‡ä»¶"):
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            relative_path = audio_file.relative_to(input_dir)
            output_filename = f"{audio_file.stem}{suffix}{audio_file.suffix}"
            output_path = output_dir / relative_path.parent / output_filename
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¤„ç†æ–‡ä»¶
            success = self.process_file(
                str(audio_file),
                str(output_path),
                denoise_only=denoise_only,
                num_steps=num_steps,
                verbose=False
            )
            
            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nå¤„ç†ç»Ÿè®¡:")
        print(f"  æ€»è®¡: {stats['total']}")
        print(f"  æˆåŠŸ: {stats['success']}")
        print(f"  å¤±è´¥: {stats['failed']}")
        
        return stats


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description='ç®€åŒ–çš„StoRMéŸ³é¢‘å¢å¼ºæ¨ç†å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
  python simple_storm_inference.py -i noisy.wav -o enhanced.wav
  
  # å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå®Œæ•´æ‰©æ•£æ¨¡å¼ï¼‰
  python simple_storm_inference.py -i noisy.wav -o enhanced.wav --denoise-only 0 --steps 30
  
  # æ‰¹é‡å¤„ç†ç›®å½•
  python simple_storm_inference.py -i noisy_dir/ -o enhanced_dir/ --batch
  
  # æŒ‡å®šæ¨¡å‹æ–‡ä»¶
  python simple_storm_inference.py -i noisy.wav -o enhanced.wav -m checkpoints/final_model.pt
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument('-i', '--input', type=str, required=True,
                       help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('-o', '--output', type=str, required=True,
                       help='è¾“å‡ºè·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰')
    
    # å¯é€‰å‚æ•°
    parser.add_argument('-m', '--model', type=str, 
                       default='checkpoints/best_model.pt',
                       help='æ¨¡å‹æ–‡ä»¶è·¯å¾„ (é»˜è®¤: checkpoints/best_model.pt)')
    
    parser.add_argument('--denoise-only', type=int, default=1,
                       help='æ˜¯å¦ä»…ä½¿ç”¨åˆ¤åˆ«æ¨¡å‹ (1=æ˜¯, 0=å¦, é»˜è®¤: 1)')
    
    parser.add_argument('--steps', type=int, default=10,
                       help='æ‰©æ•£æ­¥æ•°ï¼ˆä»…å½“denoise-only=0æ—¶æœ‰æ•ˆ, é»˜è®¤: 10)')
    
    parser.add_argument('--batch', action='store_true',
                       help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼ˆå½“è¾“å…¥ä¸ºç›®å½•æ—¶ï¼‰')
    
    parser.add_argument('--suffix', type=str, default='_enhanced',
                       help='è¾“å‡ºæ–‡ä»¶åç¼€ (é»˜è®¤: _enhanced)')
    
    parser.add_argument('--device', type=str, default=None,
                       help='è®¾å¤‡ (cuda/cpu, é»˜è®¤: è‡ªåŠ¨é€‰æ‹©)')
    
    parser.add_argument('--sr', type=int, default=16000,
                       help='ç›®æ ‡é‡‡æ ·ç‡ (é»˜è®¤: 16000)')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•° - æ”¯æŒå‘½ä»¤è¡Œ"""
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device is None:
        args.device = 'cuda' if torch.cuda.is_available() else 'cpu'
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(args.model).exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        print(f"  è¯·ç¡®ä¿å·²è®­ç»ƒæ¨¡å‹æˆ–æä¾›æ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        print(f"  å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶:")
        for model_file in Path('checkpoints').glob('*.pt'):
            print(f"    - {model_file}")
        return
    
    # åˆ›å»ºæ¨ç†å™¨
    print(f"\nğŸš€ åˆå§‹åŒ–StoRMæ¨ç†å™¨...")
    inference = SimpleStoRMInference(
        model_path=args.model,
        device=args.device,
        sr=args.sr
    )
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    input_path = Path(args.input)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input}")
        return
    
    # å¤„ç†å•ä¸ªæ–‡ä»¶
    if input_path.is_file():
        print(f"\nğŸ“„ å¤„ç†å•ä¸ªæ–‡ä»¶")
        
        # ç¡®ä¿è¾“å‡ºæ˜¯æ–‡ä»¶
        output_path = Path(args.output)
        if output_path.is_dir():
            # å¦‚æœè¾“å‡ºæ˜¯ç›®å½•ï¼Œåœ¨é‡Œé¢åˆ›å»ºåŒåæ–‡ä»¶
            output_filename = f"{input_path.stem}{args.suffix}{input_path.suffix}"
            output_path = output_path / output_filename
        
        success = inference.process_file(
            str(input_path),
            str(output_path),
            denoise_only=bool(args.denoise_only),
            num_steps=args.steps,
            verbose=True
        )
        
        if success:
            print(f"\nğŸ‰ å¤„ç†å®Œæˆ!")
            print(f"  è¾“å‡ºæ–‡ä»¶: {output_path}")
        else:
            print(f"\nâŒ å¤„ç†å¤±è´¥")
    
    # å¤„ç†ç›®å½•
    elif input_path.is_dir():
        print(f"\nğŸ“ å¤„ç†ç›®å½•")
        
        # å¦‚æœè¾“å‡ºæ˜¯æ–‡ä»¶ï¼Œè½¬æ¢ä¸ºç›®å½•
        output_path = Path(args.output)
        if output_path.suffix.lower() in ['.wav', '.mp3', '.flac'] and not args.batch:
            print(f"âš ï¸  è­¦å‘Š: è¾“å…¥æ˜¯ç›®å½•ä½†è¾“å‡ºæŒ‡å®šä¸ºå•ä¸ªæ–‡ä»¶")
            print(f"  ä½¿ç”¨ --batch å‚æ•°è¿›è¡Œæ‰¹é‡å¤„ç†")
            print(f"  æˆ–è€…å°†è¾“å‡ºæŒ‡å®šä¸ºç›®å½•")
            return
        
        # ç¡®ä¿è¾“å‡ºæ˜¯ç›®å½•
        if output_path.suffix:
            output_path = output_path.parent / output_path.stem
        
        # æ‰¹é‡å¤„ç†
        stats = inference.process_directory(
            input_dir=str(input_path),
            output_dir=str(output_path),
            denoise_only=bool(args.denoise_only),
            num_steps=args.steps,
            suffix=args.suffix
        )
        
        if stats['success'] > 0:
            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
            print(f"  è¾“å‡ºç›®å½•: {output_path}")
    
    else:
        print(f"âŒ æ— æ•ˆçš„è¾“å…¥è·¯å¾„: {args.input}")


def quick_demo():
    """å¿«é€Ÿæ¼”ç¤ºï¼ˆæ²¡æœ‰å‚æ•°æ—¶è¿è¡Œï¼‰"""
    print("ğŸµ StoRMéŸ³é¢‘å¢å¼ºæ¨ç†å™¨ - ç®€åŒ–ç‰ˆ")
    print("="*50)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¿…è¦çš„æ–‡ä»¶
    test_audio = "p232_009.wav"
    default_model = "checkpoints/best_model.pt"
    
    if Path(test_audio).exists() and Path(default_model).exists():
        print(f"\næ‰¾åˆ°æµ‹è¯•æ–‡ä»¶:")
        print(f"  éŸ³é¢‘æ–‡ä»¶: {test_audio}")
        print(f"  æ¨¡å‹æ–‡ä»¶: {default_model}")
        
        choice = input("\næ˜¯å¦è¿è¡Œæ¼”ç¤º? (y/n): ")
        if choice.lower() == 'y':
            print(f"\nè¿è¡Œæ¼”ç¤º...")
            
            inference = SimpleStoRMInference(
                model_path=default_model,
                device='cpu'
            )
            
            output_file = "demo_enhanced.wav"
            success = inference.process_file(
                test_audio,
                output_file,
                denoise_only=True,
                verbose=True
            )
            
            if success:
                print(f"\nâœ… æ¼”ç¤ºæˆåŠŸ!")
                print(f"  è¾“å‡ºæ–‡ä»¶: {output_file}")
    else:
        print(f"\nç¼ºå°‘æµ‹è¯•æ–‡ä»¶:")
        if not Path(test_audio).exists():
            print(f"  âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_audio}")
        if not Path(default_model).exists():
            print(f"  âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {default_model}")
        
        print(f"\nè¯·ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°:")
        print(f"  python simple_storm_inference.py -i <è¾“å…¥æ–‡ä»¶> -o <è¾“å‡ºæ–‡ä»¶>")
        print(f"\nä½¿ç”¨ --help æŸ¥çœ‹å®Œæ•´é€‰é¡¹")


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œæ˜¾ç¤ºæ¼”ç¤º
    if len(sys.argv) == 1:
        quick_demo()
    else:
        main()