# storm_inference_fixed.py
import os
import torch
import torchaudio
import numpy as np
from pathlib import Path
import argparse
from tqdm import tqdm
from typing import Optional, List, Union
import warnings
import sys
warnings.filterwarnings('ignore')

from storm_model import StoRMModel


class EnhancedStoRMInference:
    """
    å¢å¼ºçš„StoRMæ¨ç†å™¨
    æ”¯æŒå‘½ä»¤è¡Œå‚æ•°ã€æ‰¹é‡å¤„ç†å’Œè´¨é‡ä¼˜åŒ–
    """
    
    def __init__(self, 
                model_path: str,
                device: str = 'cuda' if torch.cuda.is_available() else 'cpu',
                sr: int = 16000,
                n_fft: int = 510,
                hop_length: int = 128,
                num_steps: int = 30,
                use_ema: bool = True):
        """
        åˆå§‹åŒ–æ¨ç†å™¨
        """
        self.device = device
        self.sr = sr
        self.n_fft = n_fft
        self.hop_length = hop_length
        self.win_length = n_fft
        self.num_steps = num_steps
        
        # åŠ è½½æ¨¡å‹
        print(f"ğŸ”§ åŠ è½½æ¨¡å‹: {model_path}")
        self.model = self._load_model(model_path, use_ema)
        self.model.eval()
        
        # åˆ›å»ºæ±‰å®çª—
        self.window = torch.hann_window(self.win_length)
        
        print(f"âœ… æ¨ç†å™¨åˆå§‹åŒ–å®Œæˆ:")
        print(f"   è®¾å¤‡: {device}")
        print(f"   é‡‡æ ·ç‡: {sr} Hz")
        print(f"   STFT: n_fft={n_fft}, hop={hop_length}")
        print(f"   æ‰©æ•£æ­¥æ•°: {num_steps}")
        print(f"   ä½¿ç”¨EMA: {use_ema}")
    
    def _load_model(self, model_path: str, use_ema: bool = True) -> StoRMModel:
        """åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹"""
        model = StoRMModel(base_channels=32).to(self.device)
        
        checkpoint = torch.load(model_path, map_location=self.device)
        
        if use_ema and 'ema_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['ema_state_dict'])
            print("   âœ… ä½¿ç”¨EMAæ¨¡å‹æƒé‡")
        elif 'model_state_dict' in checkpoint:
            model.load_state_dict(checkpoint['model_state_dict'])
            print("   âœ… ä½¿ç”¨æ™®é€šæ¨¡å‹æƒé‡")
        else:
            raise ValueError("æ£€æŸ¥ç‚¹ä¸­æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹æƒé‡")
        
        return model
    
    def _preprocess_audio(self, 
                        waveform: torch.Tensor, 
                        input_sr: int) -> torch.Tensor:
        """
        é¢„å¤„ç†éŸ³é¢‘
        """
        # ç¡®ä¿æ˜¯2Då¼ é‡ [channels, samples]
        if waveform.dim() == 1:
            waveform = waveform.unsqueeze(0)
        elif waveform.dim() == 3:
            waveform = waveform.squeeze(0)
        
        # è½¬æ¢ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = waveform.mean(dim=0, keepdim=True)
        
        # é‡é‡‡æ ·åˆ°ç›®æ ‡é‡‡æ ·ç‡
        if input_sr != self.sr:
            waveform = torchaudio.functional.resample(
                waveform, 
                orig_freq=input_sr, 
                new_freq=self.sr
            )
        
        return waveform
    
    def _stft(self, waveform: torch.Tensor) -> torch.Tensor:
        """STFTè½¬æ¢"""
        if waveform.dim() == 1:
            waveform = waveform.unsqueeze(0)
        
        stft = torch.stft(
            waveform,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_length,
            window=self.window.to(waveform.device),
            return_complex=True
        )
        
        real = stft.real.unsqueeze(1)
        imag = stft.imag.unsqueeze(1)
        
        return torch.cat([real, imag], dim=1)
    
    def _istft(self, 
                complex_spec: torch.Tensor, 
                target_length: Optional[int] = None) -> torch.Tensor:
        """ISTFTè½¬æ¢"""
        real = complex_spec[:, 0, :, :]
        imag = complex_spec[:, 1, :, :]
        
        stft_complex = torch.complex(real, imag)
        
        freq_bins = complex_spec.shape[2]
        calculated_length = (freq_bins - 1) * self.hop_length
        
        output_length = target_length or calculated_length
        
        waveform = torch.istft(
            stft_complex,
            n_fft=self.n_fft,
            hop_length=self.hop_length,
            win_length=self.win_length,
            window=self.window.to(complex_spec.device),
            length=output_length
        )
        
        return waveform
    
    def _adjust_to_multiple_of_8(self, tensor: torch.Tensor) -> torch.Tensor:
        """è°ƒæ•´å¼ é‡å°ºå¯¸ä¸º8çš„å€æ•°"""
        B, C, F, T = tensor.shape
        
        target_F = ((F + 7) // 8) * 8
        target_T = ((T + 7) // 8) * 8
        
        if F != target_F or T != target_T:
            tensor = torch.nn.functional.interpolate(
                tensor,
                size=(target_F, target_T),
                mode='bilinear',
                align_corners=False
            )
        
        return tensor
    
    def enhance(self, 
                noisy_waveform: torch.Tensor,
                input_sr: int = 16000,
                mode: str = 'quality',
                progress_callback = None) -> torch.Tensor:
        """
        å¢å¼ºéŸ³é¢‘
        """
        # è®¾ç½®å¢å¼ºå‚æ•°
        if mode == 'fast':
            denoise_only = True
            num_steps = 10
        elif mode == 'balanced':
            denoise_only = False
            num_steps = self.num_steps // 2
        else:  # 'quality'
            denoise_only = False
            num_steps = self.num_steps
        
        print(f"\nğŸ¯ å¢å¼ºæ¨¡å¼: {mode}")
        print(f"   ä»…åˆ¤åˆ«æ¨¡å‹: {denoise_only}")
        print(f"   æ‰©æ•£æ­¥æ•°: {num_steps}")
        
        with torch.no_grad():
            # 1. é¢„å¤„ç†
            if progress_callback:
                progress_callback(0.1, "é¢„å¤„ç†éŸ³é¢‘...")
            
            waveform = self._preprocess_audio(noisy_waveform, input_sr)
            original_length = waveform.shape[1]
            
            print(f"ğŸ“Š éŸ³é¢‘ä¿¡æ¯:")
            print(f"   åŸå§‹é•¿åº¦: {original_length}æ ·æœ¬ ({original_length/self.sr:.2f}ç§’)")
            
            # 2. STFT
            if progress_callback:
                progress_callback(0.2, "STFTè½¬æ¢...")
            
            stft = self._stft(waveform)
            print(f"   STFTå½¢çŠ¶: {stft.shape}")
            
            # 3. è°ƒæ•´å°ºå¯¸ä¸º8çš„å€æ•°
            if progress_callback:
                progress_callback(0.3, "è°ƒæ•´å°ºå¯¸...")
            
            stft_adjusted = self._adjust_to_multiple_of_8(stft)
            if stft_adjusted.shape != stft.shape:
                print(f"   è°ƒæ•´åSTFT: {stft_adjusted.shape}")
            
            # 4. æ¨¡å‹å¢å¼º
            if progress_callback:
                if denoise_only:
                    progress_callback(0.4, "åˆ¤åˆ«æ¨¡å‹å¢å¼º...")
                else:
                    progress_callback(0.4, "æ‰©æ•£æ¨¡å‹å¢å¼º...")
            
            stft_adjusted = stft_adjusted.to(self.device)
            
            try:
                enhanced_stft = self.model.enhance(
                    stft_adjusted, 
                    num_steps=num_steps, 
                    denoise_only=denoise_only
                )
                print(f"   âœ… å¢å¼ºæˆåŠŸ")
                print(f"   å¢å¼ºSTFT: {enhanced_stft.shape}")
                
            except Exception as e:
                print(f"   âŒ æ¨¡å‹å¢å¼ºå¤±è´¥: {e}")
                print("   âš ï¸ å›é€€åˆ°ä»…åˆ¤åˆ«æ¨¡å‹...")
                enhanced_stft = self.model.enhance(stft_adjusted, denoise_only=True)
            
            # 5. æ¢å¤åŸå§‹STFTå°ºå¯¸
            if progress_callback:
                progress_callback(0.8, "æ¢å¤å°ºå¯¸...")
            
            enhanced_stft = enhanced_stft.cpu()
            if enhanced_stft.shape[2:] != stft.shape[2:]:
                enhanced_stft = torch.nn.functional.interpolate(
                    enhanced_stft,
                    size=stft.shape[2:],
                    mode='bilinear',
                    align_corners=False
                )
            
            # 6. ISTFT
            if progress_callback:
                progress_callback(0.9, "ISTFTè½¬æ¢...")
            
            enhanced_waveform = self._istft(enhanced_stft, original_length)
            
            # 7. é•¿åº¦è°ƒæ•´
            current_length = enhanced_waveform.shape[1]
            if current_length != original_length:
                if current_length > original_length:
                    enhanced_waveform = enhanced_waveform[:, :original_length]
                else:
                    padding = torch.zeros(1, original_length - current_length)
                    enhanced_waveform = torch.cat([enhanced_waveform, padding], dim=1)
            
            print(f"   âœ… å¢å¼ºå®Œæˆ")
            print(f"   è¾“å‡ºé•¿åº¦: {enhanced_waveform.shape[1]}æ ·æœ¬ ({enhanced_waveform.shape[1]/self.sr:.2f}ç§’)")
            
            if progress_callback:
                progress_callback(1.0, "å®Œæˆ!")
            
            return enhanced_waveform.squeeze()
    
    def process_file(self, 
                    input_path: Union[str, Path],
                    output_path: Union[str, Path],
                    mode: str = 'balanced',
                    verbose: bool = True) -> bool:
        """
        å¤„ç†å•ä¸ªéŸ³é¢‘æ–‡ä»¶
        """
        if verbose:
            print(f"\n{'='*60}")
            print(f"ğŸµ å¤„ç†æ–‡ä»¶: {input_path}")
            print(f"{'='*60}")
        
        try:
            # åŠ è½½éŸ³é¢‘
            waveform, sr = torchaudio.load(str(input_path))
            if verbose:
                print(f"ğŸ“¥ åŠ è½½éŸ³é¢‘: {waveform.shape}, {sr}Hz")
            
            # è¿›åº¦å›è°ƒå‡½æ•°
            def progress_callback(progress, message):
                if verbose:
                    print(f"   [{progress*100:3.0f}%] {message}")
            
            # å¢å¼ºéŸ³é¢‘
            enhanced = self.enhance(
                waveform, 
                input_sr=sr, 
                mode=mode,
                progress_callback=progress_callback if verbose else None
            )
            
            # ç¡®ä¿æ­£ç¡®çš„ç»´åº¦
            if enhanced.dim() == 1:
                enhanced = enhanced.unsqueeze(0)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç»“æœ
            torchaudio.save(str(output_path), enhanced, self.sr)
            
            if verbose:
                # éªŒè¯
                if output_path.exists():
                    loaded, loaded_sr = torchaudio.load(str(output_path))
                    duration = loaded.shape[1] / loaded_sr
                    print(f"\nâœ… å¤„ç†æˆåŠŸ!")
                    print(f"   ğŸ’¾ ä¿å­˜åˆ°: {output_path}")
                    print(f"   â±ï¸  æ—¶é•¿: {duration:.2f}ç§’")
                    print(f"   ğŸ“Š å¤§å°: {output_path.stat().st_size / 1024:.1f}KB")
            
            return True
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥: {e}")
            if verbose:
                import traceback
                traceback.print_exc()
            return False
    
    def process_directory(self,
                        input_dir: Union[str, Path],
                        output_dir: Union[str, Path],
                        file_ext: str = '.wav',
                        mode: str = 'balanced',
                        suffix: str = '_enhanced',
                        overwrite: bool = False) -> dict:
        """
        æ‰¹é‡å¤„ç†ç›®å½•ä¸­çš„éŸ³é¢‘æ–‡ä»¶
        """
        input_dir = Path(input_dir)
        output_dir = Path(output_dir)
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # æŸ¥æ‰¾éŸ³é¢‘æ–‡ä»¶
        audio_files = list(input_dir.glob(f'*{file_ext}'))
        if not audio_files:
            audio_files = list(input_dir.rglob(f'*{file_ext}'))
        
        print(f"\nğŸ“ æ‰¹é‡å¤„ç†")
        print(f"   è¾“å…¥ç›®å½•: {input_dir}")
        print(f"   è¾“å‡ºç›®å½•: {output_dir}")
        print(f"   æ‰¾åˆ° {len(audio_files)} ä¸ªéŸ³é¢‘æ–‡ä»¶")
        print(f"   å¢å¼ºæ¨¡å¼: {mode}")
        
        # å¤„ç†ç»Ÿè®¡
        stats = {
            'total': len(audio_files),
            'success': 0,
            'failed': 0,
            'skipped': 0,
            'failed_files': []
        }
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        for audio_file in tqdm(audio_files, desc="å¤„ç†éŸ³é¢‘æ–‡ä»¶"):
            # ç”Ÿæˆè¾“å‡ºè·¯å¾„
            relative_path = audio_file.relative_to(input_dir)
            output_filename = f"{audio_file.stem}{suffix}{audio_file.suffix}"
            output_path = output_dir / relative_path.parent / output_filename
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if output_path.exists() and not overwrite:
                print(f"   â­ï¸  è·³è¿‡ (å·²å­˜åœ¨): {audio_file.name}")
                stats['skipped'] += 1
                continue
            
            # å¤„ç†æ–‡ä»¶
            success = self.process_file(
                audio_file,
                output_path,
                mode=mode,
                verbose=False
            )
            
            if success:
                stats['success'] += 1
            else:
                stats['failed'] += 1
                stats['failed_files'].append(str(audio_file))
        
        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print(f"\nğŸ“Š å¤„ç†ç»Ÿè®¡:")
        print(f"   æ€»è®¡: {stats['total']}")
        print(f"   æˆåŠŸ: {stats['success']} âœ…")
        print(f"   å¤±è´¥: {stats['failed']} âŒ")
        print(f"   è·³è¿‡: {stats['skipped']} â­ï¸")
        
        if stats['failed_files']:
            print(f"\nâŒ å¤±è´¥çš„æ–‡ä»¶:")
            for file in stats['failed_files']:
                print(f"   - {file}")
        
        return stats


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•° - ä¿®å¤ç‰ˆæœ¬"""
    parser = argparse.ArgumentParser(
        description='StoRMéŸ³é¢‘å¢å¼ºæ¨ç†å™¨',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # å¤„ç†å•ä¸ªæ–‡ä»¶
  python storm_inference.py --input noisy.wav --output enhanced.wav
  
  # æ‰¹é‡å¤„ç†ç›®å½•
  python storm_inference.py --input noisy_dir/ --output enhanced_dir/ --batch
  
  # é«˜è´¨é‡æ¨¡å¼
  python storm_inference.py --input noisy.wav --output enhanced.wav --mode quality --steps 50
  
  # å¿«é€Ÿæ¨¡å¼ï¼ˆä»…åˆ¤åˆ«æ¨¡å‹ï¼‰
  python storm_inference.py --input noisy.wav --output enhanced.wav --mode fast
  
  # æŒ‡å®šæ¨¡å‹æ–‡ä»¶
  python storm_inference.py --input noisy.wav --output enhanced.wav --model checkpoints/final_model.pt
        """
    )
    
    # è¾“å…¥è¾“å‡º
    parser.add_argument('--input', '-i', type=str, required=True,
                        help='è¾“å…¥éŸ³é¢‘æ–‡ä»¶æˆ–ç›®å½•')
    parser.add_argument('--output', '-o', type=str, required=True,
                        help='è¾“å‡ºè·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰')
    parser.add_argument('--batch', action='store_true',
                        help='æ‰¹é‡å¤„ç†æ¨¡å¼ï¼ˆå½“è¾“å…¥ä¸ºç›®å½•æ—¶ï¼‰')
    
    # æ¨¡å‹å‚æ•°
    parser.add_argument('--model', type=str, 
                        default='checkpoints/best_model.pt',
                        help='æ¨¡å‹æ£€æŸ¥ç‚¹è·¯å¾„ (é»˜è®¤: checkpoints/best_model.pt)')
    parser.add_argument('--no-ema', action='store_true',
                        help='ä¸ä½¿ç”¨EMAæ¨¡å‹æƒé‡')
    
    # å¤„ç†å‚æ•°
    parser.add_argument('--mode', type=str, default='balanced',
                        choices=['fast', 'balanced', 'quality'],
                        help='å¢å¼ºæ¨¡å¼: fast(å¿«é€Ÿ), balanced(å¹³è¡¡), quality(é«˜è´¨é‡) (é»˜è®¤: balanced)')
    parser.add_argument('--steps', type=int, default=30,
                        help='æ‰©æ•£æ­¥æ•° (é»˜è®¤: 30)')
    parser.add_argument('--suffix', type=str, default='_enhanced',
                        help='è¾“å‡ºæ–‡ä»¶åç¼€ (é»˜è®¤: _enhanced)')
    parser.add_argument('--overwrite', action='store_true',
                        help='è¦†ç›–å·²å­˜åœ¨çš„è¾“å‡ºæ–‡ä»¶')
    
    # éŸ³é¢‘å‚æ•°
    parser.add_argument('--sr', type=int, default=16000,
                        help='ç›®æ ‡é‡‡æ ·ç‡ (é»˜è®¤: 16000)')
    
    # è®¾å¤‡
    parser.add_argument('--device', type=str, default=None,
                        choices=['cpu', 'cuda', 'mps'],
                        help='è®¾å¤‡ (é»˜è®¤: è‡ªåŠ¨é€‰æ‹©)')
    
    return parser.parse_args()


def main():
    # ä½¿ç”¨ allow_abbrev=False é¿å…å‚æ•°ç¼©å†™é—®é¢˜
    parser = argparse.ArgumentParser(description='StoRMéŸ³é¢‘å¢å¼ºæ¨ç†å™¨', 
                                    allow_abbrev=False)
    
    """ä¸»å‡½æ•°"""
    args = parse_args()
    
    # è®¾ç½®è®¾å¤‡
    if args.device is None:
        if torch.cuda.is_available():
            args.device = 'cuda'
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            args.device = 'mps'
        else:
            args.device = 'cpu'
    
    # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    model_path = Path(args.model)
    if not model_path.exists():
        print(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {args.model}")
        print(f"   è¯·ç¡®ä¿å·²è®­ç»ƒæ¨¡å‹æˆ–æä¾›æ­£ç¡®çš„æ¨¡å‹è·¯å¾„")
        print(f"   å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶:")
        checkpoints_dir = Path('checkpoints')
        if checkpoints_dir.exists():
            for model_file in checkpoints_dir.rglob('*.pt'):
                print(f"    - {model_file}")
        else:
            print(f"    - checkpointsç›®å½•ä¸å­˜åœ¨")
        return
    
    # åˆ›å»ºæ¨ç†å™¨
    print(f"\nğŸš€ åˆå§‹åŒ–StoRMæ¨ç†å™¨...")
    inference = EnhancedStoRMInference(
        model_path=str(model_path),
        device=args.device,
        sr=args.sr,
        num_steps=args.steps,
        use_ema=not args.no_ema
    )
    
    # æ£€æŸ¥è¾“å…¥è·¯å¾„
    input_path = Path(args.input)
    output_path = Path(args.output)
    
    if not input_path.exists():
        print(f"âŒ è¾“å…¥è·¯å¾„ä¸å­˜åœ¨: {args.input}")
        return
    
    # å¤„ç†å•ä¸ªæ–‡ä»¶
    if input_path.is_file():
        print(f"\nğŸ“„ å¤„ç†å•ä¸ªæ–‡ä»¶æ¨¡å¼")
        success = inference.process_file(
            input_path,
            output_path,
            mode=args.mode,
            verbose=True
        )
        
        if success:
            print(f"\nğŸ‰ å•ä¸ªæ–‡ä»¶å¤„ç†å®Œæˆ!")
        else:
            print(f"\nâŒ å¤„ç†å¤±è´¥")
    
    # å¤„ç†ç›®å½•
    elif input_path.is_dir():
        print(f"\nğŸ“ å¤„ç†ç›®å½•æ¨¡å¼")
        
        # å¦‚æœè¾“å‡ºæ˜¯ç›®å½•æˆ–æŒ‡å®šäº†æ‰¹é‡æ¨¡å¼
        if output_path.suffix.lower() in ['.wav', '.mp3', '.flac'] and not args.batch:
            print(f"âš ï¸  è­¦å‘Š: è¾“å…¥æ˜¯ç›®å½•ä½†è¾“å‡ºæŒ‡å®šä¸ºå•ä¸ªæ–‡ä»¶")
            print(f"   ä½¿ç”¨ --batch å‚æ•°è¿›è¡Œæ‰¹é‡å¤„ç†")
            print(f"   æˆ–è€…å°†è¾“å‡ºæŒ‡å®šä¸ºç›®å½•")
            return
        
        # ç¡®ä¿è¾“å‡ºæ˜¯ç›®å½•
        if output_path.suffix:
            output_path = output_path.parent / output_path.stem
        
        # æ‰¹é‡å¤„ç†
        stats = inference.process_directory(
            input_dir=input_path,
            output_dir=output_path,
            mode=args.mode,
            suffix=args.suffix,
            overwrite=args.overwrite
        )
        
        if stats['success'] > 0:
            print(f"\nğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
            print(f"   è¾“å‡ºç›®å½•: {output_path}")
    
    else:
        print(f"âŒ æ— æ•ˆçš„è¾“å…¥è·¯å¾„: {args.input}")


def quick_test():
    """å¿«é€Ÿæµ‹è¯•"""
    print("å¿«é€Ÿæµ‹è¯•StoRMæ¨ç†å™¨...")
    
    # æ£€æŸ¥å¿…è¦çš„æ–‡ä»¶
    test_audio = Path('p232_009.wav')
    default_model = Path('checkpoints/best_model.pt')
    
    if test_audio.exists():
        print(f"âœ… æµ‹è¯•éŸ³é¢‘æ–‡ä»¶å­˜åœ¨: {test_audio}")
    else:
        print(f"âŒ æµ‹è¯•éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {test_audio}")
    
    if default_model.exists():
        print(f"âœ… é»˜è®¤æ¨¡å‹æ–‡ä»¶å­˜åœ¨: {default_model}")
    else:
        print(f"âŒ é»˜è®¤æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {default_model}")
        # æŸ¥æ‰¾å…¶ä»–æ¨¡å‹æ–‡ä»¶
        checkpoints_dir = Path('checkpoints')
        if checkpoints_dir.exists():
            model_files = list(checkpoints_dir.rglob('*.pt'))
            if model_files:
                print(f"   æ‰¾åˆ°å…¶ä»–æ¨¡å‹æ–‡ä»¶:")
                for model_file in model_files[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ª
                    print(f"    - {model_file}")
                if len(model_files) > 3:
                    print(f"    - ... è¿˜æœ‰{len(model_files)-3}ä¸ªæ–‡ä»¶")
    
    # ç®€å•çš„å¢å¼ºæµ‹è¯•
    if test_audio.exists():
        # å°è¯•æŸ¥æ‰¾æ¨¡å‹æ–‡ä»¶
        model_files = list(Path('checkpoints').rglob('*.pt'))
        if model_files:
            model_path = model_files[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„æ¨¡å‹
            print(f"\nä½¿ç”¨æ¨¡å‹: {model_path}")
            
            inference = EnhancedStoRMInference(
                model_path=str(model_path),
                device='cpu',
                num_steps=10
            )
            
            success = inference.process_file(
                test_audio,
                'test_enhanced.wav',
                mode='fast',
                verbose=True
            )
            
            if success:
                print(f"\nâœ… æµ‹è¯•æˆåŠŸ!")
                print(f"   è¾“å‡ºæ–‡ä»¶: test_enhanced.wav")
            else:
                print(f"\nâŒ æµ‹è¯•å¤±è´¥")
        else:
            print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ–‡ä»¶")
    else:
        print(f"\nâš ï¸  ç¼ºå°‘æµ‹è¯•æ–‡ä»¶ï¼Œè·³è¿‡æµ‹è¯•")


if __name__ == "__main__":
    # å¦‚æœæ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œè¿è¡Œæµ‹è¯•
    if len(sys.argv) == 1:
        print("ğŸš€ StoRMéŸ³é¢‘å¢å¼ºæ¨ç†å™¨")
        print("="*50)
        print("ä½¿ç”¨æ–¹æ³•: python storm_inference.py --input <è¾“å…¥> --output <è¾“å‡º>")
        print("\nå¸¸ç”¨å‘½ä»¤:")
        print("  python storm_inference.py -i noisy.wav -o enhanced.wav")
        print("  python storm_inference.py -i noisy.wav -o enhanced.wav --mode quality --steps 50")
        print("  python storm_inference.py -i noisy.wav -o enhanced.wav --model checkpoints/final_model.pt")
        print("\nè¿è¡Œå¿«é€Ÿæµ‹è¯•...")
        quick_test()
    else:
        main()